{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "chest_x-ray_images_classification_cnn_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPeogez3Zrt_",
        "colab_type": "text"
      },
      "source": [
        "**Importing the data from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZFwE_JifTce",
        "colab_type": "code",
        "outputId": "8f3b0fb6-6391-42e5-b473-504b3a7b19b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfIHkuXGgiEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# Unzip the data files\n",
        "!unzip /content/drive/My\\ Drive/chest-xray-pneumonia.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyHfr2MZZ1wk",
        "colab_type": "text"
      },
      "source": [
        "**Defining the data sources and creating the generators**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp3wD6zieD-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the data sources\n",
        "train_dir = '/content/chest_xray/train'\n",
        "val_dir = '/content/chest_xray/val'\n",
        "test_dir = '/content/chest_xray/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOoteKp_eD-x",
        "colab_type": "code",
        "outputId": "4172f2ee-e677-4ab0-ee42-c754dd7656aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# On remplace les valeurs par des valeurs entre 0 et 1\n",
        "train_datagen = ImageDataGenerator( )\n",
        "test_datagen = ImageDataGenerator( )\n",
        "\n",
        "# Create batches\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    shuffle = True,\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(val_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQhYy0kVaFYX",
        "colab_type": "text"
      },
      "source": [
        "**Building the CNN model in keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVLJ4sXJeD-1",
        "colab_type": "code",
        "outputId": "9f8fb210-f478-4453-c633-fff0793fc74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 522, 464, 16)      784       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 261, 232, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 129, 115, 16)      4112      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 57, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 31, 27, 16)        4112      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 15, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 5, 32)          8224      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              197632    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 215,889\n",
            "Trainable params: 215,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30N4MKVeeD-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6NWzLTsaK9_",
        "colab_type": "text"
      },
      "source": [
        "**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rfQfrV1eD-6",
        "colab_type": "code",
        "outputId": "7675ad18-8782-4952-f11b-e4e785e59229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "history = model.fit(train_generator, \n",
        "                              validation_data = val_generator,\n",
        "                              steps_per_epoch = len(train_generator),\n",
        "                              epochs = 5,\n",
        "                              validation_steps = len(val_generator),\n",
        "                              verbose = 1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "435/435 [==============================] - 312s 717ms/step - loss: 0.4782 - acc: 0.8499 - val_loss: 1.0732 - val_acc: 0.6875\n",
            "Epoch 2/5\n",
            "435/435 [==============================] - 309s 711ms/step - loss: 0.2325 - acc: 0.9149 - val_loss: 0.5995 - val_acc: 0.8125\n",
            "Epoch 3/5\n",
            "435/435 [==============================] - 310s 713ms/step - loss: 0.1550 - acc: 0.9421 - val_loss: 0.2395 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "435/435 [==============================] - 294s 676ms/step - loss: 0.1423 - acc: 0.9528 - val_loss: 0.4849 - val_acc: 0.8750\n",
            "Epoch 5/5\n",
            "435/435 [==============================] - 292s 671ms/step - loss: 0.1160 - acc: 0.9605 - val_loss: 0.2138 - val_acc: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lYE9-ZKjWux",
        "colab_type": "text"
      },
      "source": [
        "We used a small number of epochs to see what happens and to keep the training time low.\n",
        "Nevertheless, he model seems to be super performant.\n",
        "Let's see what happens when we use it to predict the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2KA1IpaRwr",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the performances**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K569gQPkGiDX",
        "colab_type": "code",
        "outputId": "995c252b-f875-4b22-e1ef-ee53ab0abce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "score = model.evaluate(test_generator, steps = len(test_generator), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 33s 631ms/step - loss: 2.3658 - acc: 0.6987\n",
            "acc: 69.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLIOF3QVjj0D",
        "colab_type": "text"
      },
      "source": [
        "The results are not so good, especially compared to results on the validation data. \n",
        "The validation set provided contains only 16 images, which seems to be too small compared to the training and test.\n",
        "We are going to move some files around to adjust the proportions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIDzWlNumsrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def move_from_to_n(src_dir,dst_dir, n):\n",
        "    # move files, no duplicates!\n",
        "    fs = os.listdir(src_dir)\n",
        "    assert len(fs) >= n\n",
        "    i=0\n",
        "    targ = fs[-n:]\n",
        "    # print('targ', targ)\n",
        "    for f in targ:\n",
        "        # print('move_from_to_n', i, f, src_dir, dst_dir)\n",
        "        os.rename(src_dir+'/'+f, dst_dir+'/'+f)\n",
        "        i+=1\n",
        "    print('copied',i,src_dir,dst_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB0ntR_nqpjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# we move files from the training dataset to the validation one\n",
        "move_from_to_n('/content/chest_xray/train/NORMAL', '/content/chest_xray/val/NORMAL', 300)\n",
        "move_from_to_n('/content/chest_xray/train/PNEUMONIA', '/content/chest_xray/val/PNEUMONIA', 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YxG0F-7sPng",
        "colab_type": "code",
        "outputId": "fbc26c5b-2f6e-4a89-d52c-4f86b4205455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# On remplace les valeurs par des valeurs entre 0 et 1\n",
        "train_datagen_new = ImageDataGenerator( )\n",
        "test_datagen_new = ImageDataGenerator( )\n",
        "\n",
        "# Create batches\n",
        "train_generator_new = train_datagen_new.flow_from_directory(train_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    shuffle = True,\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "val_generator_new = train_datagen_new.flow_from_directory(val_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "test_generator_new = test_datagen_new.flow_from_directory(test_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4616 images belonging to 2 classes.\n",
            "Found 616 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ZcUPphslqx",
        "colab_type": "text"
      },
      "source": [
        "To see the effect of this modification, let's see the training results with the same model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6IdNDOytJoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_new_data = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7qGi13BtPLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_new_data.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfMk12EIszXQ",
        "colab_type": "code",
        "outputId": "8c5f3023-e885-44f0-a77d-61662123d246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "history = model_new_data.fit(train_generator_new, \n",
        "                              validation_data = val_generator_new,\n",
        "                              steps_per_epoch = len(train_generator_new),\n",
        "                              epochs = 5,\n",
        "                              validation_steps = len(val_generator_new),\n",
        "                              verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "385/385 [==============================] - 319s 828ms/step - loss: 0.6441 - acc: 0.8345 - val_loss: 1.2641 - val_acc: 0.6429\n",
            "Epoch 2/5\n",
            "385/385 [==============================] - 321s 834ms/step - loss: 0.2403 - acc: 0.9123 - val_loss: 0.5757 - val_acc: 0.7955\n",
            "Epoch 3/5\n",
            "385/385 [==============================] - 324s 841ms/step - loss: 0.1695 - acc: 0.9391 - val_loss: 0.1957 - val_acc: 0.9318\n",
            "Epoch 4/5\n",
            "385/385 [==============================] - 326s 847ms/step - loss: 0.1768 - acc: 0.9372 - val_loss: 0.6793 - val_acc: 0.7987\n",
            "Epoch 5/5\n",
            "385/385 [==============================] - 325s 843ms/step - loss: 0.1338 - acc: 0.9573 - val_loss: 0.1137 - val_acc: 0.9578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMrmVHX0BEW",
        "colab_type": "text"
      },
      "source": [
        "It seems that there is a lot of variance in the validation results : let's use increase the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFp62Ldt2rI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_new_data_ex = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k96-Buma2qkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_new_data_ex.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3PCaOtu2pZX",
        "colab_type": "code",
        "outputId": "0cac4262-ebc9-49b6-8954-a0bbba236190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "history = model_new_data_ex.fit(train_generator_new, \n",
        "                              validation_data = val_generator_new,\n",
        "                              steps_per_epoch = len(train_generator_new),\n",
        "                              epochs = 15,\n",
        "                              validation_steps = len(val_generator_new),\n",
        "                              verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "385/385 [==============================] - 344s 894ms/step - loss: 0.4135 - acc: 0.8605 - val_loss: 0.3304 - val_acc: 0.8945\n",
            "Epoch 2/15\n",
            "385/385 [==============================] - 330s 858ms/step - loss: 0.2097 - acc: 0.9055 - val_loss: 0.2893 - val_acc: 0.8685\n",
            "Epoch 3/15\n",
            "385/385 [==============================] - 331s 861ms/step - loss: 0.1896 - acc: 0.9214 - val_loss: 0.3489 - val_acc: 0.8604\n",
            "Epoch 4/15\n",
            "385/385 [==============================] - 333s 864ms/step - loss: 0.1655 - acc: 0.9302 - val_loss: 0.4039 - val_acc: 0.8604\n",
            "Epoch 5/15\n",
            "385/385 [==============================] - 331s 860ms/step - loss: 0.1333 - acc: 0.9497 - val_loss: 0.4431 - val_acc: 0.8847\n",
            "Epoch 6/15\n",
            "385/385 [==============================] - 334s 867ms/step - loss: 0.1553 - acc: 0.9454 - val_loss: 0.4577 - val_acc: 0.8133\n",
            "Epoch 7/15\n",
            "385/385 [==============================] - 331s 859ms/step - loss: 0.1292 - acc: 0.9578 - val_loss: 0.3949 - val_acc: 0.9123\n",
            "Epoch 8/15\n",
            "385/385 [==============================] - 334s 868ms/step - loss: 0.1235 - acc: 0.9621 - val_loss: 0.5430 - val_acc: 0.8653\n",
            "Epoch 9/15\n",
            "385/385 [==============================] - 339s 881ms/step - loss: 0.1180 - acc: 0.9669 - val_loss: 0.5139 - val_acc: 0.9140\n",
            "Epoch 10/15\n",
            "385/385 [==============================] - 341s 886ms/step - loss: 0.0978 - acc: 0.9747 - val_loss: 0.7365 - val_acc: 0.8831\n",
            "Epoch 11/15\n",
            "385/385 [==============================] - 325s 844ms/step - loss: 0.1000 - acc: 0.9714 - val_loss: 1.0212 - val_acc: 0.8198\n",
            "Epoch 12/15\n",
            "385/385 [==============================] - 332s 863ms/step - loss: 0.0838 - acc: 0.9760 - val_loss: 0.4297 - val_acc: 0.9140\n",
            "Epoch 13/15\n",
            "385/385 [==============================] - 343s 891ms/step - loss: 0.0931 - acc: 0.9751 - val_loss: 0.7658 - val_acc: 0.9318\n",
            "Epoch 14/15\n",
            "385/385 [==============================] - 348s 903ms/step - loss: 0.0949 - acc: 0.9753 - val_loss: 0.5288 - val_acc: 0.9253\n",
            "Epoch 15/15\n",
            "385/385 [==============================] - 346s 898ms/step - loss: 0.1115 - acc: 0.9816 - val_loss: 0.7019 - val_acc: 0.8782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StW1WfN6NoIV",
        "colab_type": "text"
      },
      "source": [
        "We could push it further but it seems that the results aren't going to get better.\n",
        "Let's put some more parameters into the training process to make the best out of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VFfy4j1QhK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_new_data_pm = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOyT8r1uRAov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_new_data_pm.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa5RxTV7Oi4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we define when to stop the training, the version of the model to keep at the end (checkpoint) \n",
        "# and the evolution of the learning rate during the training\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=6, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss', mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ZW4dtHRT5v",
        "colab_type": "code",
        "outputId": "bf5edab6-2dff-4774-dfb0-3fcea7b04650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# We raise the number of epochs to let the other parameters do their job\n",
        "history = model_new_data_pm.fit(train_generator_new, \n",
        "                              validation_data = val_generator_new,\n",
        "                              steps_per_epoch = len(train_generator_new),\n",
        "                              epochs = 50,\n",
        "                              validation_steps = len(val_generator_new),\n",
        "                              callbacks=[earlyStopping, mcp_save],\n",
        "                              verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "385/385 [==============================] - 333s 866ms/step - loss: 0.5914 - acc: 0.8438 - val_loss: 0.5643 - val_acc: 0.7630 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "385/385 [==============================] - 332s 863ms/step - loss: 0.2594 - acc: 0.9079 - val_loss: 0.2293 - val_acc: 0.9286 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "385/385 [==============================] - 342s 889ms/step - loss: 0.1997 - acc: 0.9305 - val_loss: 0.2851 - val_acc: 0.8815 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "385/385 [==============================] - 337s 875ms/step - loss: 0.1737 - acc: 0.9402 - val_loss: 0.2720 - val_acc: 0.9383 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "385/385 [==============================] - 324s 842ms/step - loss: 0.1543 - acc: 0.9528 - val_loss: 0.3369 - val_acc: 0.9010 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "385/385 [==============================] - ETA: 0s - loss: 0.1379 - acc: 0.9558\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "385/385 [==============================] - 344s 894ms/step - loss: 0.1379 - acc: 0.9558 - val_loss: 0.3207 - val_acc: 0.9107 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "385/385 [==============================] - 343s 890ms/step - loss: 0.0697 - acc: 0.9814 - val_loss: 0.2169 - val_acc: 0.9513 - lr: 1.0000e-04\n",
            "Epoch 8/50\n",
            "385/385 [==============================] - 326s 847ms/step - loss: 0.0481 - acc: 0.9861 - val_loss: 0.2347 - val_acc: 0.9367 - lr: 1.0000e-04\n",
            "Epoch 9/50\n",
            "385/385 [==============================] - 325s 843ms/step - loss: 0.0373 - acc: 0.9892 - val_loss: 0.2362 - val_acc: 0.9497 - lr: 1.0000e-04\n",
            "Epoch 10/50\n",
            "385/385 [==============================] - ETA: 0s - loss: 0.0294 - acc: 0.9907\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "385/385 [==============================] - 325s 844ms/step - loss: 0.0294 - acc: 0.9907 - val_loss: 0.3211 - val_acc: 0.9448 - lr: 1.0000e-04\n",
            "Epoch 11/50\n",
            "385/385 [==============================] - 325s 843ms/step - loss: 0.0221 - acc: 0.9939 - val_loss: 0.3757 - val_acc: 0.9318 - lr: 1.0000e-05\n",
            "Epoch 12/50\n",
            "385/385 [==============================] - 314s 816ms/step - loss: 0.0205 - acc: 0.9937 - val_loss: 0.2871 - val_acc: 0.9529 - lr: 1.0000e-05\n",
            "Epoch 13/50\n",
            "385/385 [==============================] - 323s 838ms/step - loss: 0.0194 - acc: 0.9944 - val_loss: 0.3976 - val_acc: 0.9399 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k161pj4PZuOn",
        "colab_type": "text"
      },
      "source": [
        "**Saving the model and evaluating it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ZsWXDJZjKH",
        "colab_type": "code",
        "outputId": "938f9c12-e91b-499f-97bb-806e0bd57f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model_new_data_pm.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "#model.save_weights(\"model.h5\")\n",
        "\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyLApWjBaBAj",
        "colab_type": "text"
      },
      "source": [
        "**Load a model and its weights from files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BA1l0h8Zqsg",
        "colab_type": "code",
        "outputId": "9464a061-675c-4cd7-be04-f55e68de33e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open('/content/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/model.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfsFZTBoaIBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n7Orb2HaTW7",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc6Hsx0uYLyC",
        "colab_type": "code",
        "outputId": "97ee4966-4bf1-4db2-8494-d43c664c8a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "score = loaded_model.evaluate(test_generator_new, steps = len(test_generator_new), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 36s 693ms/step - loss: 1.1724 - accuracy: 0.7756\n",
            "accuracy: 77.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBPeZyZ2beOT",
        "colab_type": "code",
        "outputId": "f1c5f69f-ebc0-4464-d25b-7b21fe91d294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "score = model_new_data_pm.evaluate(test_generator_new, steps = len(test_generator_new), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 36s 700ms/step - loss: 1.9539 - acc: 0.7660\n",
            "accuracy: 76.60%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}