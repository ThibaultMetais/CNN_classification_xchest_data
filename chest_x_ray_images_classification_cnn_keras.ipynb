{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "chest_x-ray_images_classification_cnn_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPeogez3Zrt_",
        "colab_type": "text"
      },
      "source": [
        "**Importing the data from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZFwE_JifTce",
        "colab_type": "code",
        "outputId": "3d70e464-c3c0-4d13-9a30-a320db12c727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfIHkuXGgiEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# Unzip the data files\n",
        "!unzip /content/drive/My\\ Drive/chest-xray-pneumonia.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyHfr2MZZ1wk",
        "colab_type": "text"
      },
      "source": [
        "**Defining the data sources and creating the generators**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp3wD6zieD-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the data sources\n",
        "train_dir = '/content/chest_xray/train'\n",
        "val_dir = '/content/chest_xray/val'\n",
        "test_dir = '/content/chest_xray/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOoteKp_eD-x",
        "colab_type": "code",
        "outputId": "4172f2ee-e677-4ab0-ee42-c754dd7656aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator( )\n",
        "test_datagen = ImageDataGenerator( )\n",
        "\n",
        "# Create batches\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    shuffle = True,\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(val_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQhYy0kVaFYX",
        "colab_type": "text"
      },
      "source": [
        "**Building the CNN model in keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVLJ4sXJeD-1",
        "colab_type": "code",
        "outputId": "9f8fb210-f478-4453-c633-fff0793fc74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 522, 464, 16)      784       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 261, 232, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 129, 115, 16)      4112      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 57, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 31, 27, 16)        4112      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 15, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 5, 32)          8224      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              197632    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 215,889\n",
            "Trainable params: 215,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30N4MKVeeD-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6NWzLTsaK9_",
        "colab_type": "text"
      },
      "source": [
        "**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rfQfrV1eD-6",
        "colab_type": "code",
        "outputId": "7675ad18-8782-4952-f11b-e4e785e59229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "history = model.fit(train_generator, \n",
        "                              validation_data = val_generator,\n",
        "                              steps_per_epoch = len(train_generator),\n",
        "                              epochs = 5,\n",
        "                              validation_steps = len(val_generator),\n",
        "                              verbose = 1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "435/435 [==============================] - 312s 717ms/step - loss: 0.4782 - acc: 0.8499 - val_loss: 1.0732 - val_acc: 0.6875\n",
            "Epoch 2/5\n",
            "435/435 [==============================] - 309s 711ms/step - loss: 0.2325 - acc: 0.9149 - val_loss: 0.5995 - val_acc: 0.8125\n",
            "Epoch 3/5\n",
            "435/435 [==============================] - 310s 713ms/step - loss: 0.1550 - acc: 0.9421 - val_loss: 0.2395 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "435/435 [==============================] - 294s 676ms/step - loss: 0.1423 - acc: 0.9528 - val_loss: 0.4849 - val_acc: 0.8750\n",
            "Epoch 5/5\n",
            "435/435 [==============================] - 292s 671ms/step - loss: 0.1160 - acc: 0.9605 - val_loss: 0.2138 - val_acc: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lYE9-ZKjWux",
        "colab_type": "text"
      },
      "source": [
        "We used a small number of epochs to see what happens and to keep the training time low.\n",
        "Nevertheless, he model seems to be super performant.\n",
        "Let's see what happens when we use it to predict the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2KA1IpaRwr",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the performances**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K569gQPkGiDX",
        "colab_type": "code",
        "outputId": "995c252b-f875-4b22-e1ef-ee53ab0abce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "score = model.evaluate(test_generator, steps = len(test_generator), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 33s 631ms/step - loss: 2.3658 - acc: 0.6987\n",
            "acc: 69.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLIOF3QVjj0D",
        "colab_type": "text"
      },
      "source": [
        "The results are not so good, especially compared to results on the validation data. We will shuffle these sets to see how it affects the performance.\n",
        "Moreover, the validation set provided contains only 16 images, which seems to be too small compared to the training and test.\n",
        "We are going to move some files around to adjust the proportions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIDzWlNumsrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def move_from_to_n(src_dir,dst_dir, n):\n",
        "    # move files, no duplicates!\n",
        "    fs = os.listdir(src_dir)\n",
        "    assert len(fs) >= n\n",
        "    i=0\n",
        "    targ = fs[-n:]\n",
        "    # print('targ', targ)\n",
        "    for f in targ:\n",
        "        # print('move_from_to_n', i, f, src_dir, dst_dir)\n",
        "        os.rename(src_dir+'/'+f, dst_dir+'/'+f)\n",
        "        i+=1\n",
        "    print('copied',i,src_dir,dst_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB0ntR_nqpjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# we move files from the training dataset to the validation one\n",
        "move_from_to_n('/content/chest_xray/train/NORMAL', '/content/chest_xray/val/NORMAL', 150)\n",
        "move_from_to_n('/content/chest_xray/train/PNEUMONIA', '/content/chest_xray/val/PNEUMONIA', 150)\n",
        "\n",
        "move_from_to_n('/content/chest_xray/test/NORMAL', '/content/chest_xray/val/NORMAL', 150)\n",
        "move_from_to_n('/content/chest_xray/test/PNEUMONIA', '/content/chest_xray/val/PNEUMONIA', 150)\n",
        "\n",
        "move_from_to_n('/content/chest_xray/train/NORMAL', '/content/chest_xray/test/NORMAL', 150)\n",
        "move_from_to_n('/content/chest_xray/train/PNEUMONIA', '/content/chest_xray/test/PNEUMONIA', 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YxG0F-7sPng",
        "colab_type": "code",
        "outputId": "1ee2089b-ccdb-4d72-eaad-6ead566981f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen_new = ImageDataGenerator( )\n",
        "test_datagen_new = ImageDataGenerator( )\n",
        "\n",
        "# Create batches\n",
        "train_generator_new = train_datagen_new.flow_from_directory(train_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    shuffle = True,\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "val_generator_new = train_datagen_new.flow_from_directory(val_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))\n",
        "\n",
        "test_generator_new = test_datagen_new.flow_from_directory(test_dir, \n",
        "                                                    batch_size = 12,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (2090,1858))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4616 images belonging to 2 classes.\n",
            "Found 616 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ZcUPphslqx",
        "colab_type": "text"
      },
      "source": [
        "To see the effect of this modification, let's see the training results with the same model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6IdNDOytJoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_new_data = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7qGi13BtPLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_new_data.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfMk12EIszXQ",
        "colab_type": "code",
        "outputId": "bbe994bd-b965-4e1a-ce4a-72634c37f6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "history = model_new_data.fit(train_generator_new, \n",
        "                              validation_data = val_generator_new,\n",
        "                              steps_per_epoch = len(train_generator_new),\n",
        "                              epochs = 5,\n",
        "                              validation_steps = len(val_generator_new),\n",
        "                              verbose = 1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "385/385 [==============================] - 305s 792ms/step - loss: 0.4644 - acc: 0.8559 - val_loss: 0.7963 - val_acc: 0.6834\n",
            "Epoch 2/5\n",
            "385/385 [==============================] - 305s 792ms/step - loss: 0.1905 - acc: 0.9307 - val_loss: 0.4573 - val_acc: 0.8604\n",
            "Epoch 3/5\n",
            "385/385 [==============================] - 310s 805ms/step - loss: 0.1542 - acc: 0.9435 - val_loss: 0.7497 - val_acc: 0.8068\n",
            "Epoch 4/5\n",
            "385/385 [==============================] - 308s 799ms/step - loss: 0.1352 - acc: 0.9526 - val_loss: 0.4936 - val_acc: 0.8360\n",
            "Epoch 5/5\n",
            "385/385 [==============================] - 312s 809ms/step - loss: 0.1325 - acc: 0.9601 - val_loss: 1.0279 - val_acc: 0.8003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StW1WfN6NoIV",
        "colab_type": "text"
      },
      "source": [
        "Let's put some more parameters into the training process to make the best out of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VFfy4j1QhK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_new_data_pm = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', input_shape = (2090,1858,3), strides=(4, 4)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (4,4), activation = 'relu', strides=(2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOyT8r1uRAov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_new_data_pm.compile(optimizer = RMSprop (lr = 0.001),\n",
        "             loss = 'binary_crossentropy',\n",
        "             metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa5RxTV7Oi4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we define when to stop the training, the version of the model to keep at the end (checkpoint) \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=6, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss', mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ZW4dtHRT5v",
        "colab_type": "code",
        "outputId": "284e177e-5b30-4f4e-f301-76da723ef366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# We raise the number of epochs to let the other parameters do their job\n",
        "history = model_new_data_pm.fit(train_generator_new, \n",
        "                              validation_data = val_generator_new,\n",
        "                              steps_per_epoch = len(train_generator_new),\n",
        "                              epochs = 50,\n",
        "                              validation_steps = len(val_generator_new),\n",
        "                              callbacks=[earlyStopping, mcp_save],\n",
        "                              verbose = 1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "385/385 [==============================] - 299s 776ms/step - loss: 0.9061 - acc: 0.8074 - val_loss: 0.6148 - val_acc: 0.7581\n",
            "Epoch 2/50\n",
            "385/385 [==============================] - 316s 820ms/step - loss: 0.2836 - acc: 0.8852 - val_loss: 0.5264 - val_acc: 0.8701\n",
            "Epoch 3/50\n",
            "385/385 [==============================] - 325s 845ms/step - loss: 0.2094 - acc: 0.9198 - val_loss: 0.5210 - val_acc: 0.9058\n",
            "Epoch 4/50\n",
            "385/385 [==============================] - 316s 820ms/step - loss: 0.1858 - acc: 0.9309 - val_loss: 0.5524 - val_acc: 0.8653\n",
            "Epoch 5/50\n",
            "385/385 [==============================] - 313s 813ms/step - loss: 0.1576 - acc: 0.9474 - val_loss: 1.3954 - val_acc: 0.7208\n",
            "Epoch 6/50\n",
            "385/385 [==============================] - 294s 763ms/step - loss: 0.1448 - acc: 0.9530 - val_loss: 0.7230 - val_acc: 0.8474\n",
            "Epoch 7/50\n",
            "385/385 [==============================] - 294s 763ms/step - loss: 0.1107 - acc: 0.9627 - val_loss: 1.7176 - val_acc: 0.7922\n",
            "Epoch 8/50\n",
            "385/385 [==============================] - 298s 773ms/step - loss: 0.1400 - acc: 0.9573 - val_loss: 0.7137 - val_acc: 0.8036\n",
            "Epoch 9/50\n",
            "385/385 [==============================] - 299s 775ms/step - loss: 0.1210 - acc: 0.9671 - val_loss: 1.4096 - val_acc: 0.7760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k161pj4PZuOn",
        "colab_type": "text"
      },
      "source": [
        "**Saving the model and evaluating it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ZsWXDJZjKH",
        "colab_type": "code",
        "outputId": "6f76c61a-f040-4372-b0c1-c8ca1eec3b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model_new_data_pm.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "#model.save_weights(\"model.h5\")\n",
        "\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BA1l0h8Zqsg",
        "colab_type": "code",
        "outputId": "aa8e53e6-0fc5-484c-d1fd-9441c404eac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open('/content/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/model.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfsFZTBoaIBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n7Orb2HaTW7",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc6Hsx0uYLyC",
        "colab_type": "code",
        "outputId": "9ddaed89-5efc-4c5b-81f6-cebe46b36d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "score = loaded_model.evaluate(test_generator_new, steps = len(test_generator_new), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 35s 675ms/step - loss: 0.3818 - accuracy: 0.9054\n",
            "accuracy: 90.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBPeZyZ2beOT",
        "colab_type": "code",
        "outputId": "f72e82f7-7afe-42f9-dfcc-f41fc50deac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate the model fully trained (without checkpoint) on test data\n",
        "score = model_new_data_pm.evaluate(test_generator_new, steps = len(test_generator_new), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 35s 676ms/step - loss: 0.9924 - acc: 0.8462\n",
            "accuracy: 84.62%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
